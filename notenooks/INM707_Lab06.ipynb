{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTRcz+q7yLiYKKImV5EisS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgo-city/INM707-lab06/blob/main/notenooks/INM707_Lab06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "PRB_mumU56M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U \"ray[rllib]\""
      ],
      "metadata": {
        "id": "lDQvBf8--2P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwzdCCEo4M2e"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/mgo-city/INM707-lab06.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dungeon.dungeon import Dungeon, index_to_actions"
      ],
      "metadata": {
        "id": "jPE4JX0o5Z5d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "W1zkSOEx5j4Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DungeonEnv(gym.Env):\n",
        "\n",
        "    def __init__(self, render_mode=None, size=5):\n",
        "        self.size = size  # The size of the Dungeon\n",
        "\n",
        "        # Observations are dictionaries with the agent's and the target's location.\n",
        "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
        "        self.observation_space = spaces.Dict(\n",
        "            {\n",
        "                \"relative_coordinates\": spaces.Box(-size, size , shape=(2,), dtype=int),\n",
        "                \"surroundings\": spaces.Box(0, 4, shape=(5, 5), dtype=int)\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.render_mode = \"ansi\"\n",
        "\n",
        "        self.clock = None\n",
        "\n",
        "        self._dungeon_env = Dungeon(size)\n",
        "        self._dungeon_env.reset()\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # We need the following line to seed self.np_random\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        obs = self._dungeon_env.reset()\n",
        "\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        act = index_to_actions[action]\n",
        "\n",
        "        observations, reward, done = self._dungeon_env.step(act.name)\n",
        "\n",
        "        return observations, reward, done, False, {}\n",
        "\n",
        "    def render(self):\n",
        "\n",
        "        envir_with_agent = self._dungeon_env.dungeon.copy()\n",
        "        envir_with_agent[self._dungeon_env.position_agent[0], self._dungeon_env.position_agent[1]] = 4\n",
        "        \n",
        "        full_repr = \"\"\n",
        "\n",
        "        for r in range(self.size):\n",
        "            \n",
        "            line = \"\"\n",
        "            \n",
        "            for c in range(self.size):\n",
        "\n",
        "                string_repr = self._dungeon_env.dict_map_display[ envir_with_agent[r,c] ]\n",
        "                \n",
        "                line += \"{0:2}\".format(string_repr)\n",
        "\n",
        "            full_repr += line + \"\\n\"\n",
        "\n",
        "        return full_repr\n"
      ],
      "metadata": {
        "id": "-96WZ-Ul52zN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dungeon_env = DungeonEnv(size=100)\n"
      ],
      "metadata": {
        "id": "_hMyUey_IMjv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dungeon_env.render()\n",
        "dungeon_env.step(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu9H6e_WITtc",
        "outputId": "76215d18-6593-43cf-8a01-c0c85cd23745"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'relative_coordinates': array([80,  8]),\n",
              "  'surroundings': array([[0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0]], dtype=int8)},\n",
              " -1,\n",
              " False,\n",
              " False,\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray.rllib.algorithms import ppo, dqn\n",
        "from ray.tune.logger import pretty_print"
      ],
      "metadata": {
        "id": "OsLtox2e-ZHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "algo = dqn.DQN(env=DungeonEnv, config={\n",
        "    \"env_config\": {\"size\":40},  # config to pass to env class\n",
        "})\n",
        "\n",
        "while True:\n",
        "    metrics = algo.train()\n",
        "    mean_reward = metrics['episode_reward_mean']\n",
        "    print(mean_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH5E9xsY_Sfw",
        "outputId": "8e6cbcf8-b93e-45e5-83d8-8c9bc8a53f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-03-29 09:49:54,517\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "2023-03-29 09:49:55,875\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='<class '__main__.DungeonEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class '__main__.DungeonEnv'>').build()` instead. This will raise an error in the future!\n",
            "2023-03-29 09:49:55,882\tINFO algorithm_config.py:2899 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "2023-03-29 09:49:55,916\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "2023-03-29 09:49:55,930\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
            "2023-03-29 09:49:55,935\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
            "2023-03-29 09:49:59,099\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-129.22727272727272\n",
            "-90.43\n",
            "-53.1588785046729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uXRWJuiLG4fq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}